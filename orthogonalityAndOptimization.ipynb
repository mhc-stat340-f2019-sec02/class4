{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that, given a model, the parameters that best explain the data can be found by minimizing the sum squares error **(SSE)**.\n",
    "SSE was written as a function of our parameters and minimized by taking the derivative with respect to each parameter.\n",
    "\n",
    "Our end result was the following equation\n",
    "\n",
    "$$\n",
    "  \\beta_{\\text{optimal}} = (X'X)^{-1}X'y.\n",
    "$$\n",
    "\n",
    "This was a calculus approach to finding optima. \n",
    "We can also understand optima, and arrive at this same equation, by using linear algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## orthogonality\n",
    "\n",
    "Two vectors, $x$ and $y$ are perpendicular to one another, or **orthogonal**, if their inner product equals $0$\n",
    "\n",
    "$$\n",
    "    x'y = 0\n",
    "$$\n",
    "\n",
    "We can use the law of cosines to see why this is the case.\n",
    "The law of cosines says, give a triangle with edge-lengths $a$, $b$, and $c$, and the angle $\\gamma$ made by the vector $CA$ and $CB$, the edge-lengths are related like\n",
    "\n",
    "![alt text](./Triangle_with_notations_2.png)\n",
    "\n",
    "\n",
    "$$\n",
    "    C^2 = A^2+B^2 - 2AB\\cos(\\gamma)\n",
    "$$\n",
    "\n",
    "where capital letters denote the **length** of the triangle's sides.\n",
    "\n",
    "If we consider $a$ and $b$ vectors, then \n",
    "\n",
    "$$\n",
    "    c = a-b\n",
    "$$\n",
    "\n",
    "and the length of $c$ is\n",
    "\n",
    "\\begin{align}\n",
    "    c^2  = c'c &= [ a_{1} - b_{1} a_{2} - b_{2}]\n",
    "   \\left[\n",
    "     \\begin{array}{c} \n",
    "     a_{1} - b_{1}\\\\\n",
    "     a_{2} - b_{2}\\\\\n",
    "    \\end{array} \\right]\\\\\n",
    "    &= (a_{1} - b_{1})^2 + (a_{2} - b_{2})^2\\\\\n",
    "    & = (a_{1}^{2} + a_{2}^{2}) + (b_{1}^2 + b_{2}^2) - 2(a_{1}b_{1} + a_{2}b_{2})\n",
    "\\end{align}\n",
    "\n",
    "The above can be rewritten as the inner product of three vectors\n",
    "\n",
    "\\begin{align}\n",
    "   c^2 & = (a_{1}^{2} + a_{2}^{2}) + (b_{1}^2 + b_{2}^2) - 2(a_{1}b_{1} + a_{2}b_{2})\\\\\n",
    "   &=a'a + b'b - 2a'b\n",
    "\\end{align}\n",
    "then we can relate this vector equation to our original cosine law.\n",
    "\n",
    "$$\n",
    " a'a + b'b - 2a'b  = A^2+B^2 - 2AB\\cos(\\gamma)\n",
    "$$\n",
    "\n",
    "We see that $a'a$ corresponds to the the length $A$ squared and $b'b$ corresponds to the length $B$ squared.\n",
    "\n",
    "We define a vector's length\n",
    "\n",
    "$$\n",
    "||v|| = (v'v)^{1/2},\n",
    "$$\n",
    "\n",
    "and note that the length of a vector is always positive, and can only be zero if the vector has entries all zero.\n",
    "\n",
    "The last term then relates the inner product between a and b to their lengths and the cosine of the angle they make\n",
    "\n",
    "\\begin{align}\n",
    "- 2a'b &= - 2AB\\cos(\\gamma)\\\\\n",
    "   a'b &= AB\\cos(\\gamma)\\\\\n",
    "   a'b &= ||a||||b|| \\cos(\\gamma)\n",
    "\\end{align}\n",
    "\n",
    "if the inner product $a'b$ is zero\n",
    "\n",
    "\\begin{align}\n",
    "   0 &= ||a||||b|| \\cos(\\gamma)\n",
    "\\end{align}\n",
    "\n",
    "and assuming $a$ and $b$ are not zero vectors, it must be the case that $\\cos(\\gamma) = 0$ and this happens when $\\gamma = \\frac{\\pi}{2}$, a perpendicular (orthogonal) angle.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## projection\n",
    "\n",
    "A vector $b$ is a **orthogonal** projection onto $a$ if the inner product between $b-a$ and $a$ is $0$.\n",
    "\n",
    "![alt text](./scalarProjection.png)\n",
    "\n",
    "We can derive a formula for this \"green\" vector.\n",
    "The goal is to find the number $\\omega$, in the same direction as $a$, so that $b-a$ and $a$ are orthogonal.\n",
    "\n",
    "\\begin{align}\n",
    "    (b-\\omega a)'(\\omega a) = \\omega b'a - \\omega^{2} a'a &= 0\\\\\n",
    "    b'a - \\omega a'a &=0\\\\\n",
    "    \\omega &= \\frac{b'a}{a'a}\n",
    "\\end{align}\n",
    "\n",
    "This value $\\omega = b'a \\Big / a'a$ is the distance along $a$ we need to travel until $a$ and $b-a$ are orthogonal to one another.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## orthogonal projection as minimizer\n",
    "\n",
    "What does orthogonality and minima have to do with each other?\n",
    "\n",
    "Suppose we want to find the vector $p \\in S$ such that $p$ is closer to $y \\in B$ than any other vector $v \\in S$, and we assume $S \\subset B$, that $y$ and any vector $v$ cannot lie in th same space.\n",
    "\n",
    "The distance between $y$ and any vector $v$ is\n",
    "\\begin{align}\n",
    "    ||y - v|| = [(y-v)'(y-v)]^{1/2},\n",
    "\\end{align}\n",
    "\n",
    "and if a vector $p$ is closest in distance $||.||$ than it will be closes in squared distance too $||.||^{2}$.\n",
    "\n",
    "So we're searching for a vector $p$ so that \n",
    "\\begin{align}\n",
    "    ||y - v||^{2} = (y-v)'(y-v)\n",
    "\\end{align}\n",
    "\n",
    "is as small as possible.\n",
    "\n",
    "First we introduce this smallest vector $p$ without changing the above equation\n",
    "\n",
    "\\begin{align}\n",
    "    ||y - p + p - v||^{2} &= \\{[(y-p)+(p-v)]'[(y-p)+(p-v)]\\}\\\\\n",
    "                          &= (y-p)'(y-p) + (p-v)'(p-v) + 2(p-v)'(y-p)\\\\\n",
    "                          &= ||y-p||^{2} + ||p-v||^{2} + 2(p-v)'(y-p)\n",
    "\\end{align}\n",
    "\n",
    "the first two terms here cannot be changed much, but lets look at the third term.\n",
    "$p$ and $v$ are both vectors in $S$ and so their subtraction is a vector in $S$.\n",
    "$y$ is in $B$ and $p$ is in $S$.\n",
    "if we suppose $p$ is the vector such that the difference $y-p$ is orthogonal to **every** possible vector in $S$ then the third term would equal 0.\n",
    "\n",
    "The vector $p$ is smallest if and only if the difference between $y$ and $p$ is orthogonal to every vector in $S$.\n",
    "\n",
    "\n",
    "### (aside) span\n",
    "\n",
    "We can represent any vector in a space $S$ through a basis.\n",
    "A basis is a set of independent vectors such that every vector in $S$ is the weighted sum of basis vectors.\n",
    "\n",
    "Suppose $a$ is in some space $V$.\n",
    "Then a basis is a set of vectors ${v_{1},v_{2},\\cdots,v_{n}}$ such that\n",
    "\n",
    "$$\n",
    "    a = \\sum_{i=1}^{N} \\alpha_{i} v_{i}\n",
    "$$\n",
    "\n",
    "for every vector $a \\in V$.\n",
    "\n",
    "\n",
    "Returning back to our vector $p$, this vector is the one so that $y-p$ is orthogonal to every vector in $S$, or \n",
    "\n",
    "$$\n",
    "    (y-p)' \\left( \\sum_{i=1}^{N} \\alpha_{i}v_{i} \\right) = \\sum_{i=1}^{N} \\alpha_{i}  (y-p)' v_{i} = 0\n",
    "$$\n",
    "\n",
    "of $y-p$ must be orthogonal to every basis vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reframing our problem in linear algebra\n",
    "\n",
    "We can use material on orthogonal projections to help us understand the optimal $\\beta$.\n",
    "\n",
    "Our **design** matrix $X$ times $\\beta$ can be thought of as a basis.\n",
    "\n",
    "$$\n",
    "X\\beta = \\left[ \\begin{array}{c}\n",
    "x_{1,1}\\beta_{1}+ x_{1,2}\\beta_{2} + \\cdots +  \\beta_{n}x_{1,n}\\\\\n",
    "x_{2,1}\\beta_{1}+ x_{2,2}\\beta_{2} + \\cdots +  \\beta_{n}x_{2,n}\\\\\n",
    "x_{3,1}\\beta_{1}+ x_{3,2}\\beta_{2} + \\cdots +  \\beta_{n}x_{3,n}\\\\\n",
    "\\vdots\\\\\n",
    "x_{m,1} \\beta_{1}+ x_{m,2}\\beta_{2} + \\cdots +  \\beta_{n}x_{m,n}\\\\\n",
    "\\end{array}\n",
    "\\right ] = \n",
    "\\beta_{1} \\left[\n",
    "\\begin{array}{c}\n",
    "   x_{1,1}\\\\\n",
    "   x_{2,1}\\\\\n",
    "   \\vdots\\\\\n",
    "   x_{m,1}\n",
    "\\end{array}\n",
    "        \\right]\n",
    "+ \\beta_{2}\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "   x_{1,2}\\\\\n",
    "   x_{2,2}\\\\\n",
    "   \\vdots\\\\\n",
    "   x_{m,2}\n",
    "\\end{array}\n",
    "        \\right]\n",
    "+ \\cdots +\n",
    "\\beta_{n}\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "   x_{1,n}\\\\\n",
    "   x_{2,n}\\\\\n",
    "   \\vdots\\\\\n",
    "   x_{m,n}\n",
    "\\end{array}\n",
    "        \\right]\n",
    " = \\sum_{i=1}^{N} \\beta_{i} x_{;,i}        \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $y$ observations can also be considered a single $m$-dimensional vector.\n",
    "\n",
    "$$\n",
    " y = \\left[ \n",
    " \\begin{array}{c}\n",
    " y_{1}\\\\\n",
    " y_{2}\\\\\n",
    " \\vdots\\\\\n",
    " y_{m}\n",
    "\\end{array}\n",
    " \\right].\n",
    "$$\n",
    "\n",
    "Instead of asking for the $beta$ that minimizes the **SSE**, let's instead ask for the vector that is a member of the space spanned by the columns of $X$ and closest to the vector $y$.\n",
    "This could be an alternative expression for \"good fit to the data\".\n",
    "\n",
    "This best vector's (denoted $b$ for best) difference from $y$ must be orthogonal to all vectors in the space, or equivalently all vectors in the basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "(y-b)'x_{;1} & =0\\\\\n",
    "(y-b)'x_{;2} & =0\\\\\n",
    "(y-b)'x_{;3} & =0\\\\\n",
    "\\vdots       & =0\\\\\n",
    "(y-b)'x_{;1} & =0\n",
    "\\end{align}\n",
    "\n",
    "or \n",
    "\n",
    "\\begin{align}\n",
    "y'x_{;1} - b'x_{;1} & =0\\\\\n",
    "y'x_{;2} - b'x_{;2} & =0\\\\\n",
    "y'x_{;3} - b'x_{;3} & =0\\\\\n",
    "\\vdots &=0\\\\\n",
    "y'x_{;n} - b'x_{;n} & =0\\\\\n",
    "\\end{align}\n",
    "\n",
    "rearranging terms\n",
    "\n",
    "\\begin{align}\n",
    "y'x_{;1} &= b'x_{;1} \\\\\n",
    "y'x_{;2} &= b'x_{;2} \\\\\n",
    "y'x_{;3} &= b'x_{;3} \\\\\n",
    "\\vdots   &=0\\\\\n",
    "y'x_{;n} &= b'x_{;n} \\\\\n",
    "\\end{align}\n",
    "\n",
    "We can rewrite both sides of the above equation as a matrix times a vector.\n",
    "\n",
    "\\begin{align}\n",
    "X'y &= X'b\n",
    "\\end{align}\n",
    "\n",
    "We can take this equation further by remembering $b$ must be a member of the space created by the columns of $X$.\n",
    "That is $b$ is a weighted sum of the columns of $X$, for weights (let's say) $\\beta$.\n",
    "\n",
    "\\begin{align}\n",
    "  b &= \\sum_{i=1}^{N} \\beta_{i} x_{;i}\\\\\n",
    "  &= \\beta_{1} \\left[ \\begin{array}{c}\n",
    "                      x_{1,1}\\\\\n",
    "                      x_{2,1}\\\\\n",
    "                      \\vdots\\\\\n",
    "                      x_{m,1}\n",
    "                       \\end{array}\n",
    "                \\right ]\n",
    "                +\n",
    "       \\beta_{2} \\left[ \\begin{array}{c}\n",
    "                      x_{1,2}\\\\\n",
    "                      x_{2,2}\\\\\n",
    "                      \\vdots\\\\\n",
    "                      x_{m,2}\n",
    "                       \\end{array}\n",
    "                \\right ]\n",
    "                + \\cdots + \n",
    "        \\beta_{n} \\left[ \\begin{array}{c}\n",
    "                      x_{1,n}\\\\\n",
    "                      x_{2,n}\\\\\n",
    "                      \\vdots\\\\\n",
    "                      x_{m,n}\n",
    "                       \\end{array}\n",
    "                \\right ]\n",
    "        = X\\beta\n",
    "\\end{align}\n",
    "\n",
    "and the above equation now becomes\n",
    "\n",
    "\\begin{align}\n",
    "   X'y &= X'b\\\\\n",
    "   X'y &= X'X\\beta\n",
    "\\end{align}\n",
    "\n",
    "This is **exactly** the same equation as before. \n",
    "Minimizing the sum squares of error is the same as finding the vector $b$, constrained to be a weighted sum of the columns of $X$, closest to the vector $y$.\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "   \\beta = (X'X)^{-1}X'y\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hat matrix\n",
    "\n",
    "Now that we know how to compute optimal weights $(\\beta)$ for our vector $b$, we see the vector closest to $y$ is\n",
    "\n",
    "\\begin{align}\n",
    "    b &= X\\beta,\n",
    "\\end{align}\n",
    "\n",
    "but this vector is just the functional form we specified for our model, minus the error.\n",
    "The vector $b$ is used to make predictions about $y$ given data $X$, so that\n",
    "\n",
    "\\begin{align}\n",
    "    \\hat{y} &= Xb\\\\\n",
    "    \\hat{y} &= X \\left[ (X'X)^{-1}X'y\\right]\\\\\n",
    "    \\hat{y} &= \\left[X(X'X)^{-1}X'\\right] y\\\\\n",
    "\\end{align}\n",
    "\n",
    "Considered a function, the matrix \n",
    "$$\n",
    "H = \\left[X(X'X)^{-1}X'\\right]\n",
    "$$\n",
    "\n",
    "is called the **hat matrix** because it transforms $y$ into the vector $\\hat{y}$, it places the \"hat\" on $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
